{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import PIL\n",
    "import h5py\n",
    "import cv2\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '5'\n",
    "from tensorflow.keras.models import load_model\n",
    "from layers import BilinearUpSampling2D\n",
    "from loss import depth_loss_function\n",
    "from utils import predict, display_images\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argument Parser\n",
    "parser = argparse.ArgumentParser(description='High Quality Monocular Depth Estimation via Transfer Learning')\n",
    "parser.add_argument('--model', default='nyu.h5', type=str, help='Trained Keras model file.')\n",
    "parser.add_argument('--input', default='my_examples/*.jpg', type=str, help='Input filename or folder.')\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "# Custom object needed for inference and training\n",
    "start = time.time()\n",
    "custom_objects = {'BilinearUpSampling2D': BilinearUpSampling2D, 'depth_loss_function': depth_loss_function}\n",
    "\n",
    "print('Loading model...')\n",
    "\n",
    "# Load model into GPU / CPU\n",
    "model = load_model(args.model, custom_objects=custom_objects, compile=False)\n",
    "\n",
    "print('\\nModel loaded ({0}).'.format(args.model))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Heatmap Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_with_resize(image_files):\n",
    "    loaded_images = []\n",
    "    for file in image_files:\n",
    "        im = Image.open( file )\n",
    "        im = im.resize((640, 480), PIL.Image.ANTIALIAS)\n",
    "        x = np.clip(np.asarray(im, dtype=float) / 255, 0, 1)\n",
    "        loaded_images.append(x)\n",
    "    return np.stack(loaded_images, axis=0)\n",
    "\n",
    "def DepthNorm(x, maxDepth):\n",
    "    return maxDepth / x\n",
    "\n",
    "def predict(model, images, minDepth=10, maxDepth=1000, batch_size=2):\n",
    "    # Support multiple RGBs, one RGB image, even grayscale \n",
    "    if len(images.shape) < 3: images = np.stack((images,images,images), axis=2)\n",
    "    if len(images.shape) < 4: images = images.reshape((1, images.shape[0], images.shape[1], images.shape[2]))\n",
    "    # Compute predictions\n",
    "    print(images[0].shape)\n",
    "    predictions = model.predict(images, batch_size=batch_size)\n",
    "    print(predictions[0].shape)\n",
    "    # Put in expected range\n",
    "    return np.clip(DepthNorm(predictions, maxDepth=1000), minDepth, maxDepth) / maxDepth\n",
    "\n",
    "# Input images\n",
    "inputs = load_images_with_resize( glob.glob(args.input) )\n",
    "print('\\nLoaded ({0}) images of size {1}.'.format(inputs.shape[0], inputs.shape[1:]))\n",
    "\n",
    "# Compute results\n",
    "outputs = predict(model, inputs)\n",
    "\n",
    "end = time.time()\n",
    "print('It took: ', end - start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Image\n",
    "rgb_image = cv2.imread('my_examples/img-office.jpg')\n",
    "plt.figure(figsize=(10,5))\n",
    "out = plt.imshow(rgb_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Ground-Truth Image\n",
    "f = h5py.File('nyu_depth_v2_labeled.mat', 'r')\n",
    "nyu_depths = f['depths']\n",
    "nyu_depth_image = nyu_depths[0]\n",
    "nyu_depth_image = cv2.rotate(nyu_depth_image, cv2.ROTATE_90_CLOCKWISE)\n",
    "nyu_depth_image = cv2.flip(nyu_depth_image, 1)\n",
    "\n",
    "# Delete every other row and column of the array\n",
    "\n",
    "plt.imshow(nyu_depth_image, cmap='plasma', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Predicted Image\n",
    "def normalize_image(image):\n",
    "    min = np.min(image)\n",
    "    max = np.max(image)\n",
    "    return (image - min) / (max - min)\n",
    "\n",
    "def display(image):\n",
    "    print(\"Value Range:\", image.min(), \"-\", image.max())\n",
    "    print(image.shape)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    out = plt.imshow(image, cmap='plasma')\n",
    "    out = plt.colorbar()\n",
    "\n",
    "output = normalize_image(np.asarray(outputs[0])) # This output can be messed with however now\n",
    "display(output)\n",
    "# plt.savefig('results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the two images as a function between each other\n",
    "x_axis = output.flatten()\n",
    "y_axis = nyu_depth_image.flatten()\n",
    "plt.plot(x_axis, y_axis)\n",
    "plt.title('title name')\n",
    "plt.xlabel('x_axis name')\n",
    "plt.ylabel('y_axis name')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b793d7bc73bea712712f6f108b8aeff3d41b14825160568d437f9f2c3cae05f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
